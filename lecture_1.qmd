# Nature and scope of Econometrics

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

# Introductory Econometrics

## Install packages

```{r, message=FALSE}
library(wooldridge)
library(readr)
library(stargazer)
library(kableExtra)
library(quantmod)
library(xts) 
```

## Useful dataset

```{r}
earns <- read.csv("data/earns.csv")
gpa1 <- read.csv("data/gpa1.csv")
hprice1 <- read.csv("data/hprice1.csv")
hprice2 <- read.csv("data/hprice2.csv")
hprice3 <- read.csv("data/hprice3.csv")
jtrain <- read.csv("data/jtrain.csv")
nyse <- read.csv("data/nyse.csv")
phillips <- read.csv("data/phillips.csv")
rdchem <- read.csv("data/rdchem.csv")
traffic1 <- read.csv("data/traffic1.csv")
wage1 <- read.csv("data/wage1.csv")
```

## Simple regression model

### **`Example 1:` A log wage equation**

-   Load the `wage1` data and check out the documentation.

$educ$: years of education

$wage$: average hourly earnings

$lwage$: log of the average hourly earnings

-   First, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.

```{r}
plot(y = wage1$wage, x = wage1$educ, col = "black", pch = 21, bg = "orange",     
     cex=1.25, xaxt="n", frame = FALSE, main = "Wages vs. Education, 1976", 
     xlab = "years of education", ylab = "Hourly wages")
axis(side = 1, at = c(0,6,12,18))
rug(wage1$wage, side=2, col="black")
```

1.  It appears that ***on average***, more years of education, leads to higher wages.

2.  The example in the text is interested in the *return to another year of education*, or what the ***percentage*** change in wages one might expect for each additional year of education. To do so, one must use the $log($`wage`$)$. This has already been computed in the data set and is defined as `lwage`.

-   Build a linear model to estimate the relationship between the *log of wage* (`lwage`) and *education* (`educ`).

$$\widehat{log(wage)} = \beta_0 + \beta_1educ$$

```{r, echo = TRUE, warning=FALSE}
log_wage_model <- lm(lwage ~ educ, data = wage1)
```

-   Print the `summary` of the results.

```{r, echo = TRUE, warning=FALSE}
summary(log_wage_model)
```

-   Use the `stargazer` package to make beautiful table

```{r, results = 'asis', echo = TRUE, warning = FALSE, message = FALSE}
stargazer(type = "html", log_wage_model, single.row = TRUE, header = FALSE, digits = 3)
```

-   Plot the $log($`wage`$)$ vs `educ`. The blue line represents the least squares fit.

```{r, echo=TRUE}
plot(y = wage1$lwage, x = wage1$educ, main = "A Log Wage Equation", 
     col = "orange", pch = 21, bg = "black", cex=1.25,
     xlab = "years of education", ylab = "log of average hourly wages",
     xaxt="n", frame = FALSE)
axis(side = 1, at = c(0,6,12,18))
abline(log_wage_model, col = "black", lwd=2)
rug(wage1$lwage, side=2, col="black")
```

## Multiple regression analysis

### **`Example 2:` Hourly wage equation**

Check the documentation for variable information

$lwage$: log of the average hourly earnings

$educ$: years of education

$exper$: years of potential experience

$tenutre$: years with current employer

-   Plot the variables against `lwage` and compare their distributions and slope ($\beta$) of the simple regression lines

```{r, fig.height=3, echo=TRUE}
par(mfrow=c(1,3))
plot(y = wage1$lwage, x = wage1$educ, col="orange", xaxt="n", frame = FALSE, main = "years of education", xlab = "", ylab = "")
mtext(side=2, line=2.5, "Hourly wages", cex=1.25)
axis(side = 1, at = c(0,6,12,18))
abline(lm(lwage ~ educ, data=wage1), col = "black", lwd=2)
plot(y = wage1$lwage, x = wage1$exper, col="orange", xaxt="n", frame = FALSE, main = "years of experience", xlab = "", ylab = "")
axis(side = 1, at = c(0,12.5,25,37.5,50))
abline(lm(lwage ~ exper, data=wage1), col = "black", lwd=2)
plot(y = wage1$lwage, x = wage1$tenure, col="orange", xaxt="n", frame = FALSE, main = "years with employer", xlab = "", ylab = "")
axis(side = 1, at = c(0,11,22,33,44))
abline(lm(lwage ~ tenure, data=wage1), col = "black", lwd=2)
```

-   Estimate the model regressing *educ*, *exper*, and *tenure* against *log(wage)*.

$$\widehat{log(wage)} = \beta_0 + \beta_1educ + \beta_3exper + \beta_4tenure$$

```{r}
hourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)
```

-   Print the estimated model coefficients:

```{r, eval=FALSE}
coefficients(hourly_wage_model)
```

```{r, echo=TRUE}
kable(coefficients(hourly_wage_model), digits=4, col.names = "Coefficients", align = 'l')
```

-   Plot the coefficients, representing percentage impact of each variable on $log($`wage`$)$ for a quick comparison.

```{r, echo=TRUE}
barplot(sort(100*hourly_wage_model$coefficients[-1]), horiz=TRUE, las=1,
        ylab = " ", main = "Coefficients of Hourly Wage Equation")
```

## Multiple regression analysis: inference

### **`Example 3:` Hourly Wage Equation**

Using the same model estimated in **`example 3`**, examine and compare the standard errors associated with each coefficient. Like the textbook, these are contained in parenthesis next to each associated coefficient.

```{r}
summary(hourly_wage_model)
```

```{r, results='asis', echo=TRUE, warning=FALSE, message=FALSE}
stargazer(type = "html", hourly_wage_model,  single.row = TRUE, header = FALSE, digits=5)
```

For the years of experience variable, or `exper`, use coefficient and Standard Error to compute the $t$ statistic:

$$t_{exper} = \frac{0.004121}{0.001723} = 2.391$$

Fortunately, `R` includes $t$ statistics in the `summary` of model diagnostics.

```{r, eval=TRUE}
summary(hourly_wage_model)$coefficients
```

```{r, echo=TRUE}
kable(summary(hourly_wage_model)$coefficients, align="l", digits=3)
```

-   lets plot this results

```{r, fig.height=8}
par(mfrow=c(2,2))
plot(y = hourly_wage_model$residuals, x = hourly_wage_model$fitted.values , col="orange", xaxt="n", 
     frame = FALSE, main = "Fitted Values", xlab = "", ylab = "")
mtext(side=2, line=2.5, "Model Residuals", cex=1.25)
abline(0, 0, col = "black", lty=2, lwd=2)
plot(y = hourly_wage_model$residuals, x = wage1$educ, col="green", xaxt="n", 
     frame = FALSE, main = "years of education", xlab = "", ylab = "")
axis(side = 1, at = c(0,6,12,18))
abline(0, 0, col = "black", lty=2, lwd=2)
plot(y = hourly_wage_model$residuals, x = wage1$exper, col="gray", xaxt="n", 
     frame = FALSE, main = "years of experience", xlab = "", ylab = "")
mtext(side=2, line=2.5, "Model Residuals", cex=1.25)
axis(side = 1, at = c(0,12.5,25,37.5,50))
abline(0, 0, col = "black", lty=2, lwd=2)
plot(y = hourly_wage_model$residuals, x = wage1$tenure, col="blue", xaxt="n", 
     frame = FALSE, main = "years with employer", xlab = "", ylab = "")
axis(side = 1, at = c(0,11,22,33,44))
abline(0, 0, col = "black", lty=2, lwd=2)
```

-   Plot the $t$ statistics for a visual comparison:

```{r, echo=TRUE}
barplot(sort(summary(hourly_wage_model)$coefficients[-1, "t value"]), horiz=TRUE, las=1, 
        ylab = " ", main = "t statistics of Hourly Wage Equation")
```

### **`Example 4:` Effect of Job Training on Firm Scrap Rates**

-   Load the `jtrain` data set. (From H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), *Are Training Subsidies Effective? The Michigan Experience*, Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data.)

$year:$ 1987, 1988, or 1989

$union:$ =1 if unionized

$lscrap:$ Log(scrap rate per 100 items)

$hrsemp:$ (total hours training) / (total employees trained)\
$lsales:$ Log(annual sales, \$)

$lemploy:$ Log(umber of employees at plant)

-   First, use the `subset` function and it's argument by the same name to return observations which occurred in **1987** and are not **union**.

-   At the same time, use the `select` argument to return only the variables of interest for this problem.

```{r}
jtrain_subset <- subset(jtrain, subset = (year == 1987 & union == 0), select = c(year, union, lscrap, hrsemp, lsales, lemploy))
```

-   Next, test for missing values. One can "eyeball" these with R Studio's `View` function, but a more precise approach combines the `sum` and `is.na` functions to return the total number of observations equal to `NA`.

```{r}
sum(is.na(jtrain_subset))
```

-   While `R`'s `lm` function will automatically remove missing `NA` values, eliminating these manually will produce more clearly proportioned graphs for exploratory analysis. Call the `na.omit` function to remove all missing values and assign the new `data.frame` object the name **`jtrain_clean`**.

```{r}
jtrain_clean <- na.omit(jtrain_subset)
```

-   We use `jtrain_clean` to plot the variables of interest against `lscrap`. Visually observe the respective distributions for each variable, and compare the slope ($\beta$) of the simple regression lines.

```{r, echo=TRUE, fig.height=3}
par(mfrow=c(1,3))
point_size <- 1.60
plot(y = jtrain_clean$lscrap, x = jtrain_clean$hrsemp, frame = FALSE, 
main = "Total (hours/employees) trained", ylab = "", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
mtext(side=2, line=2, "Log(scrap rate)", cex=1.25)
abline(lm(lscrap ~ hrsemp, data=jtrain_clean), col = "black", lwd=2)
plot(y = jtrain_clean$lscrap, x = jtrain_clean$lsales, frame = FALSE, main = "Log(annual sales $)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lsales, data=jtrain_clean), col = "black", lwd=2)
plot(y = jtrain_clean$lscrap, x = jtrain_clean$lemploy, frame = FALSE, main = "Log(# employees at plant)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lemploy, data=jtrain_clean), col = "black", lwd=2)
```

-   Now create the linear model regressing `hrsemp`(total hours training/total employees trained), `lsales`(log of annual sales), and `lemploy`(the log of the number of the employees), against `lscrap`(the log of the scrape rate).

$$lscrap = \alpha + \beta_1 hrsemp + \beta_2 lsales + \beta_3 lemploy$$

```{r}
linear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)
```

-   Finally, print the complete summary diagnostics of the model.

```{r, eval=TRUE, warning=FALSE, message=FALSE}
summary(linear_model)
```

-   Use `stargazer` to create representative table

```{r, results='asis', echo=TRUE, warning=FALSE, message=FALSE}
stargazer(type = "html", linear_model, single.row = TRUE, header = FALSE, digits=5)
```

```{r, echo=TRUE, eval=TRUE}
coefficient <- coef(linear_model)[-1]
 confidence <- confint(linear_model, level = 0.95)[-1,]
graph <- drop(barplot(coefficient, ylim = range(c(confidence)),
              main = "Coefficients & 95% C.I. of variables on Firm Scrap Rates"))  
arrows(graph, coefficient, graph, confidence[,1], angle=90, length=0.55, col="black", lwd=2)
arrows(graph, coefficient, graph, confidence[,2], angle=90, length=0.55, col="black", lwd=2)
```

## Chapter 5: Multiple Regression Analysis: OLS Asymptotics

### **`Example:` Housing Prices and Distance From an Incinerator**

-   We will use the `hprice3` data set.

$lprice:$ Log(selling price)

$ldist:$ Log(distance from house to incinerator, feet)

$larea:$ Log(square footage of house)

-   Graph the prices of housing against distance from an incinerator:

```{r, echo=TRUE, fig.align='center'}
par(mfrow=c(1,2))
plot(y = hprice3$price, x = hprice3$dist, main = " ", xlab = "Distance to Incinerator in feet", ylab = "Selling Price",  frame = FALSE, pch = 21, bg = "lightgrey")
abline(lm(price ~ dist, data=hprice3), col = "black", lwd=2)
```

-   Next, model the $log($`price`$)$ against the $log($`dist`$)$ to estimate the percentage relationship between the two.

$$price = \alpha + \beta_1 dist$$

```{r}
price_dist_model <- lm(lprice ~ ldist, data = hprice3)
```

-   Create another model that controls for "quality" variables, such as square footage `area` per house.

$$price = \alpha + \beta_1 dist + \beta_2 area$$

```{r}
price_area_model <- lm(lprice ~ ldist + larea, data = hprice3)
```

-   Compare the coefficients of both models. Notice that adding `area` improves the quality of the model, but also reduces the coefficient size of `dist`.

```{r, eval=TRUE}
summary(price_dist_model)
summary(price_area_model)
```

-   Use *`stargazer`* for better table

```{r, results='asis', echo=TRUE, warning=FALSE, message=FALSE}
stargazer(type = "html",price_dist_model, price_area_model,  single.row = TRUE, header = FALSE, digits=5)
```

-   Graphing illustrates the larger coefficient for `area`

```{r, echo=FALSE}
par(mfrow=c(1,2))
point_size <- 0.80
plot(y = hprice3$lprice, x = hprice3$ldist, frame = FALSE, 
main = "Log(distance from incinerator)", ylab = "", xlab="", 
pch = 21, bg = "lightgrey", cex=point_size)
mtext(side=2, line=2, "Log( selling price )", cex=1.25)
abline(lm(lprice ~ ldist, data=hprice3), col = "black", lwd=2)
plot(y = hprice3$lprice, x = hprice3$larea, frame = FALSE, main = "Log(square footage of house)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lprice ~ larea, data=hprice3), col = "black", lwd=2)
```
