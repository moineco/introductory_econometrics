[
  {
    "objectID": "lecture_1.html#install-packages",
    "href": "lecture_1.html#install-packages",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.1 Install packages",
    "text": "2.1 Install packages\n\nlibrary(wooldridge)\nlibrary(readr)\nlibrary(stargazer)\nlibrary(kableExtra)\nlibrary(quantmod)\nlibrary(xts)"
  },
  {
    "objectID": "lecture_1.html#useful-dataset",
    "href": "lecture_1.html#useful-dataset",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.2 Useful dataset",
    "text": "2.2 Useful dataset\n\nearns <- read.csv(\"data/earns.csv\")\ngpa1 <- read.csv(\"data/gpa1.csv\")\nhprice1 <- read.csv(\"data/hprice1.csv\")\nhprice2 <- read.csv(\"data/hprice2.csv\")\nhprice3 <- read.csv(\"data/hprice3.csv\")\njtrain <- read.csv(\"data/jtrain.csv\")\nnyse <- read.csv(\"data/nyse.csv\")\nphillips <- read.csv(\"data/phillips.csv\")\nrdchem <- read.csv(\"data/rdchem.csv\")\ntraffic1 <- read.csv(\"data/traffic1.csv\")\nwage1 <- read.csv(\"data/wage1.csv\")"
  },
  {
    "objectID": "lecture_1.html#simple-regression-model",
    "href": "lecture_1.html#simple-regression-model",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.3 Simple regression model",
    "text": "2.3 Simple regression model\n\n2.3.1 Example 1: A log wage equation\n\nLoad the wage1 data and check out the documentation.\n\n\\(educ\\): years of education\n\\(wage\\): average hourly earnings\n\\(lwage\\): log of the average hourly earnings\n\nFirst, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.\n\n\nplot(y = wage1$wage, x = wage1$educ, col = \"black\", pch = 21, bg = \"orange\",     \n     cex=1.25, xaxt=\"n\", frame = FALSE, main = \"Wages vs. Education, 1976\", \n     xlab = \"years of education\", ylab = \"Hourly wages\")\naxis(side = 1, at = c(0,6,12,18))\nrug(wage1$wage, side=2, col=\"black\")\n\n\n\n\n\nIt appears that on average, more years of education, leads to higher wages.\nThe example in the text is interested in the return to another year of education, or what the percentage change in wages one might expect for each additional year of education. To do so, one must use the \\(log(\\)wage\\()\\). This has already been computed in the data set and is defined as lwage.\n\n\nBuild a linear model to estimate the relationship between the log of wage (lwage) and education (educ).\n\n\\[\\widehat{log(wage)} = \\beta_0 + \\beta_1educ\\]\n\nlog_wage_model <- lm(lwage ~ educ, data = wage1)\n\n\nPrint the summary of the results.\n\n\nsummary(log_wage_model)\n\n\nCall:\nlm(formula = lwage ~ educ, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.21158 -0.36393 -0.07263  0.29712  1.52339 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.583773   0.097336   5.998 3.74e-09 ***\neduc        0.082744   0.007567  10.935  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4801 on 524 degrees of freedom\nMultiple R-squared:  0.1858,    Adjusted R-squared:  0.1843 \nF-statistic: 119.6 on 1 and 524 DF,  p-value: < 2.2e-16\n\n\n\nUse the stargazer package to make beautiful table\n\nstargazer(type = \"html\", log_wage_model, single.row = TRUE, header = FALSE, digits = 3)\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n\n\n\n\n\n\neduc\n\n\n0.083*** (0.008)\n\n\n\n\nConstant\n\n\n0.584*** (0.097)\n\n\n\n\n\n\n\n\nObservations\n\n\n526\n\n\n\n\nR2\n\n\n0.186\n\n\n\n\nAdjusted R2\n\n\n0.184\n\n\n\n\nResidual Std. Error\n\n\n0.480 (df = 524)\n\n\n\n\nF Statistic\n\n\n119.582*** (df = 1; 524)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nPlot the \\(log(\\)wage\\()\\) vs educ. The blue line represents the least squares fit.\n\n\nplot(y = wage1$lwage, x = wage1$educ, main = \"A Log Wage Equation\", \n     col = \"orange\", pch = 21, bg = \"black\", cex=1.25,\n     xlab = \"years of education\", ylab = \"log of average hourly wages\",\n     xaxt=\"n\", frame = FALSE)\naxis(side = 1, at = c(0,6,12,18))\nabline(log_wage_model, col = \"black\", lwd=2)\nrug(wage1$lwage, side=2, col=\"black\")"
  },
  {
    "objectID": "lecture_1.html#multiple-regression-analysis",
    "href": "lecture_1.html#multiple-regression-analysis",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.4 Multiple regression analysis",
    "text": "2.4 Multiple regression analysis\n\n2.4.1 Example 2: Hourly wage equation\nCheck the documentation for variable information\n\\(lwage\\): log of the average hourly earnings\n\\(educ\\): years of education\n\\(exper\\): years of potential experience\n\\(tenutre\\): years with current employer\n\nPlot the variables against lwage and compare their distributions and slope (\\(\\beta\\)) of the simple regression lines\n\n\npar(mfrow=c(1,3))\nplot(y = wage1$lwage, x = wage1$educ, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years of education\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Hourly wages\", cex=1.25)\naxis(side = 1, at = c(0,6,12,18))\nabline(lm(lwage ~ educ, data=wage1), col = \"black\", lwd=2)\nplot(y = wage1$lwage, x = wage1$exper, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years of experience\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,12.5,25,37.5,50))\nabline(lm(lwage ~ exper, data=wage1), col = \"black\", lwd=2)\nplot(y = wage1$lwage, x = wage1$tenure, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years with employer\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,11,22,33,44))\nabline(lm(lwage ~ tenure, data=wage1), col = \"black\", lwd=2)\n\n\n\n\n\nEstimate the model regressing educ, exper, and tenure against log(wage).\n\n\\[\\widehat{log(wage)} = \\beta_0 + \\beta_1educ + \\beta_3exper + \\beta_4tenure\\]\n\nhourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)\n\n\nPrint the estimated model coefficients:\n\n\ncoefficients(hourly_wage_model)\n\n\nkable(coefficients(hourly_wage_model), digits=4, col.names = \"Coefficients\", align = 'l')\n\n\n\n \n  \n      \n    Coefficients \n  \n \n\n  \n    (Intercept) \n    0.2844 \n  \n  \n    educ \n    0.0920 \n  \n  \n    exper \n    0.0041 \n  \n  \n    tenure \n    0.0221 \n  \n\n\n\n\n\n\nPlot the coefficients, representing percentage impact of each variable on \\(log(\\)wage\\()\\) for a quick comparison.\n\n\nbarplot(sort(100*hourly_wage_model$coefficients[-1]), horiz=TRUE, las=1,\n        ylab = \" \", main = \"Coefficients of Hourly Wage Equation\")"
  },
  {
    "objectID": "lecture_1.html#multiple-regression-analysis-inference",
    "href": "lecture_1.html#multiple-regression-analysis-inference",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.5 Multiple regression analysis: inference",
    "text": "2.5 Multiple regression analysis: inference\n\n2.5.1 Example 3: Hourly Wage Equation\nUsing the same model estimated in example 3, examine and compare the standard errors associated with each coefficient. Like the textbook, these are contained in parenthesis next to each associated coefficient.\n\nsummary(hourly_wage_model)\n\n\nCall:\nlm(formula = lwage ~ educ + exper + tenure, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05802 -0.29645 -0.03265  0.28788  1.42809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.284360   0.104190   2.729  0.00656 ** \neduc        0.092029   0.007330  12.555  < 2e-16 ***\nexper       0.004121   0.001723   2.391  0.01714 *  \ntenure      0.022067   0.003094   7.133 3.29e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4409 on 522 degrees of freedom\nMultiple R-squared:  0.316, Adjusted R-squared:  0.3121 \nF-statistic: 80.39 on 3 and 522 DF,  p-value: < 2.2e-16\n\n\nstargazer(type = \"html\", hourly_wage_model,  single.row = TRUE, header = FALSE, digits=5)\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n\n\n\n\n\n\neduc\n\n\n0.09203*** (0.00733)\n\n\n\n\nexper\n\n\n0.00412** (0.00172)\n\n\n\n\ntenure\n\n\n0.02207*** (0.00309)\n\n\n\n\nConstant\n\n\n0.28436*** (0.10419)\n\n\n\n\n\n\n\n\nObservations\n\n\n526\n\n\n\n\nR2\n\n\n0.31601\n\n\n\n\nAdjusted R2\n\n\n0.31208\n\n\n\n\nResidual Std. Error\n\n\n0.44086 (df = 522)\n\n\n\n\nF Statistic\n\n\n80.39092*** (df = 3; 522)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\nFor the years of experience variable, or exper, use coefficient and Standard Error to compute the \\(t\\) statistic:\n\\[t_{exper} = \\frac{0.004121}{0.001723} = 2.391\\]\nFortunately, R includes \\(t\\) statistics in the summary of model diagnostics.\n\nsummary(hourly_wage_model)$coefficients\n\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 0.284359541 0.104190379  2.729230 6.562466e-03\neduc        0.092028988 0.007329923 12.555246 8.824197e-32\nexper       0.004121109 0.001723277  2.391437 1.713562e-02\ntenure      0.022067218 0.003093649  7.133070 3.294407e-12\n\n\n\nkable(summary(hourly_wage_model)$coefficients, align=\"l\", digits=3)\n\n\n\n \n  \n      \n    Estimate \n    Std. Error \n    t value \n    Pr(>|t|) \n  \n \n\n  \n    (Intercept) \n    0.284 \n    0.104 \n    2.729 \n    0.007 \n  \n  \n    educ \n    0.092 \n    0.007 \n    12.555 \n    0.000 \n  \n  \n    exper \n    0.004 \n    0.002 \n    2.391 \n    0.017 \n  \n  \n    tenure \n    0.022 \n    0.003 \n    7.133 \n    0.000 \n  \n\n\n\n\n\n\nlets plot this results\n\n\npar(mfrow=c(2,2))\nplot(y = hourly_wage_model$residuals, x = hourly_wage_model$fitted.values , col=\"orange\", xaxt=\"n\", \n     frame = FALSE, main = \"Fitted Values\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Model Residuals\", cex=1.25)\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$educ, col=\"green\", xaxt=\"n\", \n     frame = FALSE, main = \"years of education\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,6,12,18))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$exper, col=\"gray\", xaxt=\"n\", \n     frame = FALSE, main = \"years of experience\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Model Residuals\", cex=1.25)\naxis(side = 1, at = c(0,12.5,25,37.5,50))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$tenure, col=\"blue\", xaxt=\"n\", \n     frame = FALSE, main = \"years with employer\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,11,22,33,44))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\n\n\n\n\n\nPlot the \\(t\\) statistics for a visual comparison:\n\n\nbarplot(sort(summary(hourly_wage_model)$coefficients[-1, \"t value\"]), horiz=TRUE, las=1, \n        ylab = \" \", main = \"t statistics of Hourly Wage Equation\")\n\n\n\n\n\n\n2.5.2 Example 4: Effect of Job Training on Firm Scrap Rates\n\nLoad the jtrain data set. (From H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), Are Training Subsidies Effective? The Michigan Experience, Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data.)\n\n\\(year:\\) 1987, 1988, or 1989\n\\(union:\\) =1 if unionized\n\\(lscrap:\\) Log(scrap rate per 100 items)\n\\(hrsemp:\\) (total hours training) / (total employees trained)\n\\(lsales:\\) Log(annual sales, $)\n\\(lemploy:\\) Log(umber of employees at plant)\n\nFirst, use the subset function and it’s argument by the same name to return observations which occurred in 1987 and are not union.\nAt the same time, use the select argument to return only the variables of interest for this problem.\n\n\njtrain_subset <- subset(jtrain, subset = (year == 1987 & union == 0), select = c(year, union, lscrap, hrsemp, lsales, lemploy))\n\n\nNext, test for missing values. One can “eyeball” these with R Studio’s View function, but a more precise approach combines the sum and is.na functions to return the total number of observations equal to NA.\n\n\nsum(is.na(jtrain_subset))\n\n[1] 156\n\n\n\nWhile R’s lm function will automatically remove missing NA values, eliminating these manually will produce more clearly proportioned graphs for exploratory analysis. Call the na.omit function to remove all missing values and assign the new data.frame object the name jtrain_clean.\n\n\njtrain_clean <- na.omit(jtrain_subset)\n\n\nWe use jtrain_clean to plot the variables of interest against lscrap. Visually observe the respective distributions for each variable, and compare the slope (\\(\\beta\\)) of the simple regression lines.\n\n\npar(mfrow=c(1,3))\npoint_size <- 1.60\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$hrsemp, frame = FALSE, \nmain = \"Total (hours/employees) trained\", ylab = \"\", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nmtext(side=2, line=2, \"Log(scrap rate)\", cex=1.25)\nabline(lm(lscrap ~ hrsemp, data=jtrain_clean), col = \"black\", lwd=2)\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$lsales, frame = FALSE, main = \"Log(annual sales $)\", ylab = \" \", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nabline(lm(lscrap ~ lsales, data=jtrain_clean), col = \"black\", lwd=2)\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$lemploy, frame = FALSE, main = \"Log(# employees at plant)\", ylab = \" \", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nabline(lm(lscrap ~ lemploy, data=jtrain_clean), col = \"black\", lwd=2)\n\n\n\n\n\nNow create the linear model regressing hrsemp(total hours training/total employees trained), lsales(log of annual sales), and lemploy(the log of the number of the employees), against lscrap(the log of the scrape rate).\n\n\\[lscrap = \\alpha + \\beta_1 hrsemp + \\beta_2 lsales + \\beta_3 lemploy\\]\n\nlinear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)\n\n\nFinally, print the complete summary diagnostics of the model.\n\n\nsummary(linear_model)\n\n\nCall:\nlm(formula = lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6301 -0.7523 -0.4016  0.8697  2.8273 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.45837    5.68677   2.191   0.0380 *\nhrsemp      -0.02927    0.02280  -1.283   0.2111  \nlsales      -0.96203    0.45252  -2.126   0.0436 *\nlemploy      0.76147    0.40743   1.869   0.0734 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.376 on 25 degrees of freedom\nMultiple R-squared:  0.2624,    Adjusted R-squared:  0.1739 \nF-statistic: 2.965 on 3 and 25 DF,  p-value: 0.05134\n\n\n\nUse stargazer to create representative table\n\nstargazer(type = \"html\", linear_model, single.row = TRUE, header = FALSE, digits=5)\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlscrap\n\n\n\n\n\n\n\n\nhrsemp\n\n\n-0.02927 (0.02280)\n\n\n\n\nlsales\n\n\n-0.96203** (0.45252)\n\n\n\n\nlemploy\n\n\n0.76147* (0.40743)\n\n\n\n\nConstant\n\n\n12.45837** (5.68677)\n\n\n\n\n\n\n\n\nObservations\n\n\n29\n\n\n\n\nR2\n\n\n0.26243\n\n\n\n\nAdjusted R2\n\n\n0.17392\n\n\n\n\nResidual Std. Error\n\n\n1.37604 (df = 25)\n\n\n\n\nF Statistic\n\n\n2.96504* (df = 3; 25)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\ncoefficient <- coef(linear_model)[-1]\n confidence <- confint(linear_model, level = 0.95)[-1,]\ngraph <- drop(barplot(coefficient, ylim = range(c(confidence)),\n              main = \"Coefficients & 95% C.I. of variables on Firm Scrap Rates\"))  \narrows(graph, coefficient, graph, confidence[,1], angle=90, length=0.55, col=\"black\", lwd=2)\narrows(graph, coefficient, graph, confidence[,2], angle=90, length=0.55, col=\"black\", lwd=2)"
  },
  {
    "objectID": "lecture_1.html#chapter-5-multiple-regression-analysis-ols-asymptotics",
    "href": "lecture_1.html#chapter-5-multiple-regression-analysis-ols-asymptotics",
    "title": "1  Nature and scope of Econometrics",
    "section": "2.6 Chapter 5: Multiple Regression Analysis: OLS Asymptotics",
    "text": "2.6 Chapter 5: Multiple Regression Analysis: OLS Asymptotics\n\n2.6.1 Example: Housing Prices and Distance From an Incinerator\n\nWe will use the hprice3 data set.\n\n\\(lprice:\\) Log(selling price)\n\\(ldist:\\) Log(distance from house to incinerator, feet)\n\\(larea:\\) Log(square footage of house)\n\nGraph the prices of housing against distance from an incinerator:\n\n\npar(mfrow=c(1,2))\nplot(y = hprice3$price, x = hprice3$dist, main = \" \", xlab = \"Distance to Incinerator in feet\", ylab = \"Selling Price\",  frame = FALSE, pch = 21, bg = \"lightgrey\")\nabline(lm(price ~ dist, data=hprice3), col = \"black\", lwd=2)\n\n\n\n\n\n\n\n\n\nNext, model the \\(log(\\)price\\()\\) against the \\(log(\\)dist\\()\\) to estimate the percentage relationship between the two.\n\n\\[price = \\alpha + \\beta_1 dist\\]\n\nprice_dist_model <- lm(lprice ~ ldist, data = hprice3)\n\n\nCreate another model that controls for “quality” variables, such as square footage area per house.\n\n\\[price = \\alpha + \\beta_1 dist + \\beta_2 area\\]\n\nprice_area_model <- lm(lprice ~ ldist + larea, data = hprice3)\n\n\nCompare the coefficients of both models. Notice that adding area improves the quality of the model, but also reduces the coefficient size of dist.\n\n\nsummary(price_dist_model)\n\n\nCall:\nlm(formula = lprice ~ ldist, data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22356 -0.28076 -0.05527  0.27992  1.29332 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.25750    0.47383  17.427  < 2e-16 ***\nldist        0.31722    0.04811   6.594 1.78e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4117 on 319 degrees of freedom\nMultiple R-squared:  0.1199,    Adjusted R-squared:  0.1172 \nF-statistic: 43.48 on 1 and 319 DF,  p-value: 1.779e-10\n\nsummary(price_area_model)\n\n\nCall:\nlm(formula = lprice ~ ldist + larea, data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23380 -0.18820 -0.01723  0.21751  0.86039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.49394    0.49065   7.121 7.18e-12 ***\nldist        0.19623    0.03816   5.142 4.77e-07 ***\nlarea        0.78368    0.05358  14.625  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3188 on 318 degrees of freedom\nMultiple R-squared:  0.4738,    Adjusted R-squared:  0.4705 \nF-statistic: 143.2 on 2 and 318 DF,  p-value: < 2.2e-16\n\n\n\nUse stargazer for better table\n\nstargazer(type = \"html\",price_dist_model, price_area_model,  single.row = TRUE, header = FALSE, digits=5)\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlprice\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nldist\n\n\n0.31722*** (0.04811)\n\n\n0.19623*** (0.03816)\n\n\n\n\nlarea\n\n\n\n\n0.78368*** (0.05358)\n\n\n\n\nConstant\n\n\n8.25750*** (0.47383)\n\n\n3.49394*** (0.49065)\n\n\n\n\n\n\n\n\nObservations\n\n\n321\n\n\n321\n\n\n\n\nR2\n\n\n0.11994\n\n\n0.47385\n\n\n\n\nAdjusted R2\n\n\n0.11718\n\n\n0.47054\n\n\n\n\nResidual Std. Error\n\n\n0.41170 (df = 319)\n\n\n0.31883 (df = 318)\n\n\n\n\nF Statistic\n\n\n43.47673*** (df = 1; 319)\n\n\n143.19470*** (df = 2; 318)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nGraphing illustrates the larger coefficient for area"
  },
  {
    "objectID": "lecture_2.html#estimation-of-parameters",
    "href": "lecture_2.html#estimation-of-parameters",
    "title": "2  Statistical Inference",
    "section": "2.2 Estimation of parameters",
    "text": "2.2 Estimation of parameters"
  },
  {
    "objectID": "lecture_2.html#testing-of-hypotheses",
    "href": "lecture_2.html#testing-of-hypotheses",
    "title": "2  Statistical Inference",
    "section": "2.3 Testing of hypotheses",
    "text": "2.3 Testing of hypotheses"
  },
  {
    "objectID": "lecture_2.html#defining-statistical-hypotheses",
    "href": "lecture_2.html#defining-statistical-hypotheses",
    "title": "2  Statistical Inference",
    "section": "2.4 Defining statistical hypotheses",
    "text": "2.4 Defining statistical hypotheses"
  },
  {
    "objectID": "lecture_2.html#distributions-of-test-statistics",
    "href": "lecture_2.html#distributions-of-test-statistics",
    "title": "2  Statistical Inference",
    "section": "2.5 Distributions of test statistics",
    "text": "2.5 Distributions of test statistics"
  },
  {
    "objectID": "lecture_2.html#testing-hypotheses-related-to-population-parameters",
    "href": "lecture_2.html#testing-hypotheses-related-to-population-parameters",
    "title": "2  Statistical Inference",
    "section": "2.6 Testing hypotheses related to population parameters",
    "text": "2.6 Testing hypotheses related to population parameters"
  },
  {
    "objectID": "lecture_2.html#type-i-and-type-ii-errors-power-of-a-test",
    "href": "lecture_2.html#type-i-and-type-ii-errors-power-of-a-test",
    "title": "2  Statistical Inference",
    "section": "2.7 Type-I and Type-II errors; Power of a test",
    "text": "2.7 Type-I and Type-II errors; Power of a test"
  },
  {
    "objectID": "lecture_2.html#tests-for-comparing-parameters-from-two-samples",
    "href": "lecture_2.html#tests-for-comparing-parameters-from-two-samples",
    "title": "2  Statistical Inference",
    "section": "2.8 Tests for comparing parameters from two samples",
    "text": "2.8 Tests for comparing parameters from two samples"
  }
]