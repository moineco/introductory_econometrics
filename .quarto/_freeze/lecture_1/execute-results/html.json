{
  "hash": "fe88d7d981aff681ea9872f6f760332d",
  "result": {
    "markdown": "# Nature and scope of Econometrics\n\n\n\n\n::: {.cell}\n\n:::\n\n\n# Introductory Econometrics\n\n## Install packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nlibrary(readr)\nlibrary(stargazer)\nlibrary(kableExtra)\nlibrary(quantmod)\nlibrary(xts) \n```\n:::\n\n\n## Useful dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nearns <- read.csv(\"data/earns.csv\")\ngpa1 <- read.csv(\"data/gpa1.csv\")\nhprice1 <- read.csv(\"data/hprice1.csv\")\nhprice2 <- read.csv(\"data/hprice2.csv\")\nhprice3 <- read.csv(\"data/hprice3.csv\")\njtrain <- read.csv(\"data/jtrain.csv\")\nnyse <- read.csv(\"data/nyse.csv\")\nphillips <- read.csv(\"data/phillips.csv\")\nrdchem <- read.csv(\"data/rdchem.csv\")\ntraffic1 <- read.csv(\"data/traffic1.csv\")\nwage1 <- read.csv(\"data/wage1.csv\")\n```\n:::\n\n\n## Simple regression model\n\n### **`Example 1:` A log wage equation**\n\n-   Load the `wage1` data and check out the documentation.\n\n$educ$: years of education\n\n$wage$: average hourly earnings\n\n$lwage$: log of the average hourly earnings\n\n-   First, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = wage1$wage, x = wage1$educ, col = \"black\", pch = 21, bg = \"orange\",     \n     cex=1.25, xaxt=\"n\", frame = FALSE, main = \"Wages vs. Education, 1976\", \n     xlab = \"years of education\", ylab = \"Hourly wages\")\naxis(side = 1, at = c(0,6,12,18))\nrug(wage1$wage, side=2, col=\"black\")\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n1.  It appears that ***on average***, more years of education, leads to higher wages.\n\n2.  The example in the text is interested in the *return to another year of education*, or what the ***percentage*** change in wages one might expect for each additional year of education. To do so, one must use the $log($`wage`$)$. This has already been computed in the data set and is defined as `lwage`.\n\n-   Build a linear model to estimate the relationship between the *log of wage* (`lwage`) and *education* (`educ`).\n\n$$\\widehat{log(wage)} = \\beta_0 + \\beta_1educ$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_wage_model <- lm(lwage ~ educ, data = wage1)\n```\n:::\n\n\n-   Print the `summary` of the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(log_wage_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = lwage ~ educ, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.21158 -0.36393 -0.07263  0.29712  1.52339 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.583773   0.097336   5.998 3.74e-09 ***\neduc        0.082744   0.007567  10.935  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4801 on 524 degrees of freedom\nMultiple R-squared:  0.1858,\tAdjusted R-squared:  0.1843 \nF-statistic: 119.6 on 1 and 524 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   Use the `stargazer` package to make beautiful table\n\n\n\n```{.r .cell-code}\nstargazer(type = \"html\", log_wage_model, single.row = TRUE, header = FALSE, digits = 3)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"1\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>lwage</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">educ</td><td>0.083<sup>***</sup> (0.008)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.584<sup>***</sup> (0.097)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>526</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.186</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.184</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>0.480 (df = 524)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>119.582<sup>***</sup> (df = 1; 524)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n\n-   Plot the $log($`wage`$)$ vs `educ`. The blue line represents the least squares fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = wage1$lwage, x = wage1$educ, main = \"A Log Wage Equation\", \n     col = \"orange\", pch = 21, bg = \"black\", cex=1.25,\n     xlab = \"years of education\", ylab = \"log of average hourly wages\",\n     xaxt=\"n\", frame = FALSE)\naxis(side = 1, at = c(0,6,12,18))\nabline(log_wage_model, col = \"black\", lwd=2)\nrug(wage1$lwage, side=2, col=\"black\")\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Multiple regression analysis\n\n### **`Example 2:` Hourly wage equation**\n\nCheck the documentation for variable information\n\n$lwage$: log of the average hourly earnings\n\n$educ$: years of education\n\n$exper$: years of potential experience\n\n$tenutre$: years with current employer\n\n-   Plot the variables against `lwage` and compare their distributions and slope ($\\beta$) of the simple regression lines\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3))\nplot(y = wage1$lwage, x = wage1$educ, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years of education\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Hourly wages\", cex=1.25)\naxis(side = 1, at = c(0,6,12,18))\nabline(lm(lwage ~ educ, data=wage1), col = \"black\", lwd=2)\nplot(y = wage1$lwage, x = wage1$exper, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years of experience\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,12.5,25,37.5,50))\nabline(lm(lwage ~ exper, data=wage1), col = \"black\", lwd=2)\nplot(y = wage1$lwage, x = wage1$tenure, col=\"orange\", xaxt=\"n\", frame = FALSE, main = \"years with employer\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,11,22,33,44))\nabline(lm(lwage ~ tenure, data=wage1), col = \"black\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n-   Estimate the model regressing *educ*, *exper*, and *tenure* against *log(wage)*.\n\n$$\\widehat{log(wage)} = \\beta_0 + \\beta_1educ + \\beta_3exper + \\beta_4tenure$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)\n```\n:::\n\n\n-   Print the estimated model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefficients(hourly_wage_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(coefficients(hourly_wage_model), digits=4, col.names = \"Coefficients\", align = 'l')\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> Coefficients </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> 0.2844 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> educ </td>\n   <td style=\"text-align:left;\"> 0.0920 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> exper </td>\n   <td style=\"text-align:left;\"> 0.0041 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tenure </td>\n   <td style=\"text-align:left;\"> 0.0221 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n-   Plot the coefficients, representing percentage impact of each variable on $log($`wage`$)$ for a quick comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbarplot(sort(100*hourly_wage_model$coefficients[-1]), horiz=TRUE, las=1,\n        ylab = \" \", main = \"Coefficients of Hourly Wage Equation\")\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Multiple regression analysis: inference\n\n### **`Example 3:` Hourly Wage Equation**\n\nUsing the same model estimated in **`example 3`**, examine and compare the standard errors associated with each coefficient. Like the textbook, these are contained in parenthesis next to each associated coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(hourly_wage_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = lwage ~ educ + exper + tenure, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05802 -0.29645 -0.03265  0.28788  1.42809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.284360   0.104190   2.729  0.00656 ** \neduc        0.092029   0.007330  12.555  < 2e-16 ***\nexper       0.004121   0.001723   2.391  0.01714 *  \ntenure      0.022067   0.003094   7.133 3.29e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4409 on 522 degrees of freedom\nMultiple R-squared:  0.316,\tAdjusted R-squared:  0.3121 \nF-statistic: 80.39 on 3 and 522 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n```{.r .cell-code}\nstargazer(type = \"html\", hourly_wage_model,  single.row = TRUE, header = FALSE, digits=5)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"1\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>lwage</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">educ</td><td>0.09203<sup>***</sup> (0.00733)</td></tr>\n<tr><td style=\"text-align:left\">exper</td><td>0.00412<sup>**</sup> (0.00172)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>0.02207<sup>***</sup> (0.00309)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.28436<sup>***</sup> (0.10419)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>526</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.31601</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.31208</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>0.44086 (df = 522)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>80.39092<sup>***</sup> (df = 3; 522)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n\nFor the years of experience variable, or `exper`, use coefficient and Standard Error to compute the $t$ statistic:\n\n$$t_{exper} = \\frac{0.004121}{0.001723} = 2.391$$\n\nFortunately, `R` includes $t$ statistics in the `summary` of model diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(hourly_wage_model)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 0.284359541 0.104190379  2.729230 6.562466e-03\neduc        0.092028988 0.007329923 12.555246 8.824197e-32\nexper       0.004121109 0.001723277  2.391437 1.713562e-02\ntenure      0.022067218 0.003093649  7.133070 3.294407e-12\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(summary(hourly_wage_model)$coefficients, align=\"l\", digits=3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> Estimate </th>\n   <th style=\"text-align:left;\"> Std. Error </th>\n   <th style=\"text-align:left;\"> t value </th>\n   <th style=\"text-align:left;\"> Pr(&gt;|t|) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> 0.284 </td>\n   <td style=\"text-align:left;\"> 0.104 </td>\n   <td style=\"text-align:left;\"> 2.729 </td>\n   <td style=\"text-align:left;\"> 0.007 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> educ </td>\n   <td style=\"text-align:left;\"> 0.092 </td>\n   <td style=\"text-align:left;\"> 0.007 </td>\n   <td style=\"text-align:left;\"> 12.555 </td>\n   <td style=\"text-align:left;\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> exper </td>\n   <td style=\"text-align:left;\"> 0.004 </td>\n   <td style=\"text-align:left;\"> 0.002 </td>\n   <td style=\"text-align:left;\"> 2.391 </td>\n   <td style=\"text-align:left;\"> 0.017 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tenure </td>\n   <td style=\"text-align:left;\"> 0.022 </td>\n   <td style=\"text-align:left;\"> 0.003 </td>\n   <td style=\"text-align:left;\"> 7.133 </td>\n   <td style=\"text-align:left;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n-   lets plot this results\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(y = hourly_wage_model$residuals, x = hourly_wage_model$fitted.values , col=\"orange\", xaxt=\"n\", \n     frame = FALSE, main = \"Fitted Values\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Model Residuals\", cex=1.25)\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$educ, col=\"green\", xaxt=\"n\", \n     frame = FALSE, main = \"years of education\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,6,12,18))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$exper, col=\"gray\", xaxt=\"n\", \n     frame = FALSE, main = \"years of experience\", xlab = \"\", ylab = \"\")\nmtext(side=2, line=2.5, \"Model Residuals\", cex=1.25)\naxis(side = 1, at = c(0,12.5,25,37.5,50))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\nplot(y = hourly_wage_model$residuals, x = wage1$tenure, col=\"blue\", xaxt=\"n\", \n     frame = FALSE, main = \"years with employer\", xlab = \"\", ylab = \"\")\naxis(side = 1, at = c(0,11,22,33,44))\nabline(0, 0, col = \"black\", lty=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n-   Plot the $t$ statistics for a visual comparison:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbarplot(sort(summary(hourly_wage_model)$coefficients[-1, \"t value\"]), horiz=TRUE, las=1, \n        ylab = \" \", main = \"t statistics of Hourly Wage Equation\")\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### **`Example 4:` Effect of Job Training on Firm Scrap Rates**\n\n-   Load the `jtrain` data set. (From H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), *Are Training Subsidies Effective? The Michigan Experience*, Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data.)\n\n$year:$ 1987, 1988, or 1989\n\n$union:$ =1 if unionized\n\n$lscrap:$ Log(scrap rate per 100 items)\n\n$hrsemp:$ (total hours training) / (total employees trained)\\\n$lsales:$ Log(annual sales, \\$)\n\n$lemploy:$ Log(umber of employees at plant)\n\n-   First, use the `subset` function and it's argument by the same name to return observations which occurred in **1987** and are not **union**.\n\n-   At the same time, use the `select` argument to return only the variables of interest for this problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njtrain_subset <- subset(jtrain, subset = (year == 1987 & union == 0), select = c(year, union, lscrap, hrsemp, lsales, lemploy))\n```\n:::\n\n\n-   Next, test for missing values. One can \"eyeball\" these with R Studio's `View` function, but a more precise approach combines the `sum` and `is.na` functions to return the total number of observations equal to `NA`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(jtrain_subset))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 156\n```\n:::\n:::\n\n\n-   While `R`'s `lm` function will automatically remove missing `NA` values, eliminating these manually will produce more clearly proportioned graphs for exploratory analysis. Call the `na.omit` function to remove all missing values and assign the new `data.frame` object the name **`jtrain_clean`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njtrain_clean <- na.omit(jtrain_subset)\n```\n:::\n\n\n-   We use `jtrain_clean` to plot the variables of interest against `lscrap`. Visually observe the respective distributions for each variable, and compare the slope ($\\beta$) of the simple regression lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,3))\npoint_size <- 1.60\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$hrsemp, frame = FALSE, \nmain = \"Total (hours/employees) trained\", ylab = \"\", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nmtext(side=2, line=2, \"Log(scrap rate)\", cex=1.25)\nabline(lm(lscrap ~ hrsemp, data=jtrain_clean), col = \"black\", lwd=2)\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$lsales, frame = FALSE, main = \"Log(annual sales $)\", ylab = \" \", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nabline(lm(lscrap ~ lsales, data=jtrain_clean), col = \"black\", lwd=2)\nplot(y = jtrain_clean$lscrap, x = jtrain_clean$lemploy, frame = FALSE, main = \"Log(# employees at plant)\", ylab = \" \", xlab=\"\", pch = 21, bg = \"lightgrey\", cex=point_size)\nabline(lm(lscrap ~ lemploy, data=jtrain_clean), col = \"black\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n-   Now create the linear model regressing `hrsemp`(total hours training/total employees trained), `lsales`(log of annual sales), and `lemploy`(the log of the number of the employees), against `lscrap`(the log of the scrape rate).\n\n$$lscrap = \\alpha + \\beta_1 hrsemp + \\beta_2 lsales + \\beta_3 lemploy$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)\n```\n:::\n\n\n-   Finally, print the complete summary diagnostics of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(linear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6301 -0.7523 -0.4016  0.8697  2.8273 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.45837    5.68677   2.191   0.0380 *\nhrsemp      -0.02927    0.02280  -1.283   0.2111  \nlsales      -0.96203    0.45252  -2.126   0.0436 *\nlemploy      0.76147    0.40743   1.869   0.0734 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.376 on 25 degrees of freedom\nMultiple R-squared:  0.2624,\tAdjusted R-squared:  0.1739 \nF-statistic: 2.965 on 3 and 25 DF,  p-value: 0.05134\n```\n:::\n:::\n\n\n-   Use `stargazer` to create representative table\n\n\n\n```{.r .cell-code}\nstargazer(type = \"html\", linear_model, single.row = TRUE, header = FALSE, digits=5)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"1\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>lscrap</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">hrsemp</td><td>-0.02927 (0.02280)</td></tr>\n<tr><td style=\"text-align:left\">lsales</td><td>-0.96203<sup>**</sup> (0.45252)</td></tr>\n<tr><td style=\"text-align:left\">lemploy</td><td>0.76147<sup>*</sup> (0.40743)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>12.45837<sup>**</sup> (5.68677)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>29</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.26243</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.17392</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>1.37604 (df = 25)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>2.96504<sup>*</sup> (df = 3; 25)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefficient <- coef(linear_model)[-1]\n confidence <- confint(linear_model, level = 0.95)[-1,]\ngraph <- drop(barplot(coefficient, ylim = range(c(confidence)),\n              main = \"Coefficients & 95% C.I. of variables on Firm Scrap Rates\"))  \narrows(graph, coefficient, graph, confidence[,1], angle=90, length=0.55, col=\"black\", lwd=2)\narrows(graph, coefficient, graph, confidence[,2], angle=90, length=0.55, col=\"black\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## Chapter 5: Multiple Regression Analysis: OLS Asymptotics\n\n### **`Example:` Housing Prices and Distance From an Incinerator**\n\n-   We will use the `hprice3` data set.\n\n$lprice:$ Log(selling price)\n\n$ldist:$ Log(distance from house to incinerator, feet)\n\n$larea:$ Log(square footage of house)\n\n-   Graph the prices of housing against distance from an incinerator:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(y = hprice3$price, x = hprice3$dist, main = \" \", xlab = \"Distance to Incinerator in feet\", ylab = \"Selling Price\",  frame = FALSE, pch = 21, bg = \"lightgrey\")\nabline(lm(price ~ dist, data=hprice3), col = \"black\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n-   Next, model the $log($`price`$)$ against the $log($`dist`$)$ to estimate the percentage relationship between the two.\n\n$$price = \\alpha + \\beta_1 dist$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_dist_model <- lm(lprice ~ ldist, data = hprice3)\n```\n:::\n\n\n-   Create another model that controls for \"quality\" variables, such as square footage `area` per house.\n\n$$price = \\alpha + \\beta_1 dist + \\beta_2 area$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_area_model <- lm(lprice ~ ldist + larea, data = hprice3)\n```\n:::\n\n\n-   Compare the coefficients of both models. Notice that adding `area` improves the quality of the model, but also reduces the coefficient size of `dist`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(price_dist_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = lprice ~ ldist, data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22356 -0.28076 -0.05527  0.27992  1.29332 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.25750    0.47383  17.427  < 2e-16 ***\nldist        0.31722    0.04811   6.594 1.78e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4117 on 319 degrees of freedom\nMultiple R-squared:  0.1199,\tAdjusted R-squared:  0.1172 \nF-statistic: 43.48 on 1 and 319 DF,  p-value: 1.779e-10\n```\n:::\n\n```{.r .cell-code}\nsummary(price_area_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = lprice ~ ldist + larea, data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23380 -0.18820 -0.01723  0.21751  0.86039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.49394    0.49065   7.121 7.18e-12 ***\nldist        0.19623    0.03816   5.142 4.77e-07 ***\nlarea        0.78368    0.05358  14.625  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3188 on 318 degrees of freedom\nMultiple R-squared:  0.4738,\tAdjusted R-squared:  0.4705 \nF-statistic: 143.2 on 2 and 318 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   Use *`stargazer`* for better table\n\n\n\n```{.r .cell-code}\nstargazer(type = \"html\",price_dist_model, price_area_model,  single.row = TRUE, header = FALSE, digits=5)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\">lprice</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">ldist</td><td>0.31722<sup>***</sup> (0.04811)</td><td>0.19623<sup>***</sup> (0.03816)</td></tr>\n<tr><td style=\"text-align:left\">larea</td><td></td><td>0.78368<sup>***</sup> (0.05358)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>8.25750<sup>***</sup> (0.47383)</td><td>3.49394<sup>***</sup> (0.49065)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>321</td><td>321</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.11994</td><td>0.47385</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.11718</td><td>0.47054</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>0.41170 (df = 319)</td><td>0.31883 (df = 318)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>43.47673<sup>***</sup> (df = 1; 319)</td><td>143.19470<sup>***</sup> (df = 2; 318)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n\n-   Graphing illustrates the larger coefficient for `area`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture_1_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "lecture_1_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}